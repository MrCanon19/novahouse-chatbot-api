# ğŸ¤– Local AI Setup - Ollama + Continue + Qwen

Konfiguracja lokalnego asystenta AI dla tego projektu uÅ¼ywajÄ…c **Ollama** z modelem **Qwen2.5-coder:7b**.

## âœ… Status - GOTOWE DO UÅ»YCIA!

- âœ… Ollama zainstalowana
- âœ… Model `qwen2.5-coder:7b` pobrany (4.7 GB)
- âœ… Model `nomic-embed-text` pobrany (274 MB) - dla embeddings
- âœ… Serwer Ollama dziaÅ‚a na `localhost:11434`
- âœ… Continue skonfigurowane w `~/.continue/config.json`
- âœ… Context providers wÅ‚Ä…czone (code, docs, diff, terminal, problems, folder, codebase)
- âœ… PrzykÅ‚adowe prompty w `.vscode/continue-prompts.md`

**ğŸ‰ Wszystko gotowe - otwÃ³rz VS Code i naciÅ›nij `Cmd+L` Å¼eby zaczÄ…Ä‡!**

## ğŸš€ Quick Start

### 1. SprawdÅº status Ollama

```bash
# Lista modeli
ollama list

# Test modelu
ollama run qwen2.5-coder:7b "Write a hello world in Python"
```

### 2. VS Code - Continue Extension

1. Zainstaluj rozszerzenie **Continue** w VS Code
2. Konfiguracja jest juÅ¼ gotowa w `.vscode/continue_config.json`
3. OtwÃ³rz panel Continue: `Cmd+L` (Mac) lub `Ctrl+L` (Windows/Linux)
4. Zacznij zadawaÄ‡ pytania o kod!

### 3. PrzykÅ‚ady uÅ¼ycia

**W panelu Continue moÅ¼esz pisaÄ‡:**

- "WyjaÅ›nij co robi funkcja `process_chat_message` w `src/routes/chatbot.py`"
- "Dodaj nowy endpoint `/api/stats` ktÃ³ry zwraca statystyki leadÃ³w"
- "ZnajdÅº wszystkie Ğ¼ĞµÑÑ‚Ğ° gdzie uÅ¼ywamy OpenAI API"
- "Napisz testy dla `src/routes/backup.py`"
- "Zrefaktoruj funkcjÄ™ `run_auto_migration` Å¼eby byÅ‚a bardziej czytelna"

**Tab autocomplete:**
- Zaczyna pisaÄ‡ kod, model automatycznie podpowiada dalszy ciÄ…g
- NaciÅ›nij `Tab` Å¼eby zaakceptowaÄ‡ podpowiedÅº

## ğŸ”§ Zaawansowane

### Restart serwera Ollama

```bash
# JeÅ›li coÅ› nie dziaÅ‚a, zrestartuj serwer
pkill ollama
ollama serve > /dev/null 2>&1 &
```

### Zmiana modelu

MoÅ¼esz uÅ¼yÄ‡ innych modeli, np.:

```bash
# Mniejszy, szybszy model
ollama pull qwen2.5-coder:1.5b

# WiÄ™kszy, dokÅ‚adniejszy model
ollama pull qwen2.5-coder:32b
```

NastÄ™pnie zmieÅ„ w `.vscode/continue_config.json`:
```json
{
  "model": "qwen2.5-coder:1.5b"
}
```

### Context Providers

Konfiguracja wÅ‚Ä…cza automatyczne pobieranie kontekstu z:
- âœ… **code** - aktualnie otwarty kod
- âœ… **docs** - dokumentacja projektu
- âœ… **diff** - niezatwierdzone zmiany git
- âœ… **terminal** - output z terminala
- âœ… **problems** - bÅ‚Ä™dy i ostrzeÅ¼enia
- âœ… **folder** - struktura folderÃ³w
- âœ… **codebase** - przeszukiwanie caÅ‚ego repo

## ğŸ“Š Model Info

**Qwen2.5-coder:7b**
- Rozmiar: 4.7 GB
- Parametry: 7.6B
- Kwantyzacja: Q4_K_M (zoptymalizowana dla szybkoÅ›ci)
- Specjalizacja: Programowanie (Python, JavaScript, Go, Rust, itp.)
- DziaÅ‚a 100% lokalnie (bez internetu, bez kosztÃ³w API)

## ğŸ†˜ Troubleshooting

### Ollama nie odpowiada

```bash
# SprawdÅº czy serwer dziaÅ‚a
curl http://localhost:11434/api/tags

# JeÅ›li nie, uruchom:
ollama serve
```

### Model wolno generuje

- UÅ¼yj mniejszego modelu: `qwen2.5-coder:1.5b`
- Zamknij inne aplikacje Å¼eby zwolniÄ‡ RAM
- Model korzysta z GPU jeÅ›li dostÄ™pne (M1/M2 Mac automatycznie)

### Continue nie widzi modelu

1. SprawdÅº czy Ollama dziaÅ‚a: `ollama list`
2. PrzeÅ‚aduj VS Code: `Cmd+Shift+P` â†’ "Reload Window"
3. SprawdÅº logi Continue: `Cmd+Shift+P` â†’ "Continue: Show Logs"

## ğŸ”— WiÄ™cej info

- [Ollama Docs](https://ollama.com/library/qwen2.5-coder)
- [Continue Docs](https://docs.continue.dev/)
- [Qwen2.5-coder GitHub](https://github.com/QwenLM/Qwen2.5-Coder)

---

**Notatka:** Ten setup jest juÅ¼ skonfigurowany dla tego projektu. Po zainstalowaniu rozszerzenia Continue w VS Code wszystko powinno dziaÅ‚aÄ‡ automatycznie! ğŸ‰
